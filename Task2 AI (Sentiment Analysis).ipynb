{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  **Install necessary packages and libraries**"
      ],
      "metadata": {
        "id": "uQtUuQGPoV2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment transformers gensim nltk sklearn\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "from gensim import corpora, models\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "PPvoemdL3q3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c7ed74-0780-4854-afe4-f07bdd819914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping Amazon reviews**"
      ],
      "metadata": {
        "id": "31aY3_wwrgTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_amazon_reviews(url, headers):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    reviews = soup.find_all('span', {'data-hook': 'review-body'})\n",
        "    reviews_list = [review.text.strip() for review in reviews]\n",
        "    return reviews_list\n"
      ],
      "metadata": {
        "id": "dqJvceyL3-cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text cleaning functions**"
      ],
      "metadata": {
        "id": "NiCkJA7jrX0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    return text.lower().strip()"
      ],
      "metadata": {
        "id": "lUNYCBsC4LcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vader Sentiment Analysis**"
      ],
      "metadata": {
        "id": "_CI2bzMvoubY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_vader(review):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = analyzer.polarity_scores(review)\n",
        "    if sentiment_score['compound'] >= 0.05:\n",
        "        return 'positive', sentiment_score['compound']\n",
        "    elif sentiment_score['compound'] <= -0.05:\n",
        "        return 'negative', sentiment_score['compound']\n",
        "    else:\n",
        "        return 'neutral', sentiment_score['compound']"
      ],
      "metadata": {
        "id": "HpVVX68u4nE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT-based Sentiment Analysis**"
      ],
      "metadata": {
        "id": "J9-scTafq_od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_review_with_bert(review):\n",
        "    classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "    result = classifier(review)\n",
        "    return result[0]['label'], result[0]['score']\n"
      ],
      "metadata": {
        "id": "OFDQvb-r4qf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling using LDA**"
      ],
      "metadata": {
        "id": "7fRTTq0Jq50g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_topic_modeling(reviews):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    texts = [[word for word in simple_preprocess(review) if word not in stop_words] for review in reviews]\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=15)\n",
        "    topics = lda_model.print_topics(num_words=4)\n",
        "    return topics\n"
      ],
      "metadata": {
        "id": "Wr9Hq2JF4u9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert the results to a structured JSON output**"
      ],
      "metadata": {
        "id": "pLrjpBMkqz4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_json_output(review, sentiment, score, themes):\n",
        "    output = {\n",
        "        \"review_text\": review,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"sentiment_score\": score,\n",
        "        \"themes\": themes\n",
        "    }\n",
        "    return json.dumps(output)"
      ],
      "metadata": {
        "id": "N7ri0igg4yhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model training function**"
      ],
      "metadata": {
        "id": "66LB13J7quYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bert_model(train_dataset, eval_dataset):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "    train_encodings = tokenizer([example[\"text\"] for example in train_dataset], truncation=True, padding=True)\n",
        "    eval_encodings = tokenizer([example[\"text\"] for example in eval_dataset], truncation=True, padding=True)\n",
        "\n",
        "    class MyDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = MyDataset(train_encodings, [example[\"label\"] for example in train_dataset])\n",
        "    eval_dataset = MyDataset(eval_encodings, [example[\"label\"] for example in eval_dataset])\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "Yr65afB_412B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics calculation**"
      ],
      "metadata": {
        "id": "-_Z1t1nnqi4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n"
      ],
      "metadata": {
        "id": "uuCXRDtW47ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example usage**"
      ],
      "metadata": {
        "id": "MMoU8_3YqYCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Replace this URL with the actual product page you want to scrape\n",
        "    url = 'https://www.amazon.com/product-reviews/B07XJ8C8F5/'\n",
        "    headers = {'User-Agent': 'Your user agent string'}\n",
        "    reviews_list = scrape_amazon_reviews(url, headers)\n",
        "\n",
        "    # Clean and analyze the reviews\n",
        "    cleaned_reviews = [clean_text(review) for review in reviews_list]\n",
        "\n",
        "    for review in cleaned_reviews:\n",
        "        sentiment, score = analyze_sentiment_vader(review)\n",
        "        topics = perform_topic_modeling([review])\n",
        "        json_output = create_json_output(review, sentiment, score, topics)\n",
        "        print(json_output)\n"
      ],
      "metadata": {
        "id": "Mku07tZD4-xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Placeholder for testing metrics calculation**"
      ],
      "metadata": {
        "id": "Z6szCPV0v1-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of actual labels (y_true) and predicted labels (y_pred) for testing\n",
        "y_true = ['positive', 'negative', 'neutral', 'positive', 'negative']  # Example true labels\n",
        "y_pred = ['positive', 'neutral', 'neutral', 'positive', 'negative']   # Example predicted labels\n",
        "\n",
        "# Converting these string labels to numeric values for the model (e.g., 0 = negative, 1 = neutral, 2 = positive)\n",
        "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "y_true_numeric = [label_mapping[label] for label in y_true]\n",
        "y_pred_numeric = [label_mapping[label] for label in y_pred]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true_numeric, y_pred_numeric)\n",
        "precision = precision_score(y_true_numeric, y_pred_numeric, average='weighted')\n",
        "recall = recall_score(y_true_numeric, y_pred_numeric, average='weighted')\n",
        "f1 = f1_score(y_true_numeric, y_pred_numeric, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSwDImkbncSR",
        "outputId": "4824073e-526b-4c53-bf4c-f16fa4c1c18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n",
            "Precision: 0.9\n",
            "Recall: 0.8\n",
            "F1 Score: 0.8\n"
          ]
        }
      ]
    }
  ]
}